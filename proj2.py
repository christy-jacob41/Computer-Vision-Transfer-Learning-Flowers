# -*- coding: utf-8 -*-
"""proj2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ACks0FVWMeozffwskflrX9RtrSGTxyyH
"""

import itertools
import os
import cv2

import matplotlib.pylab as plt
import numpy as np

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_datasets as tfds

print("TF version:", tf.__version__)
print("Hub version:", hub.__version__)
print("", "GPU used" if tf.config.list_physical_devices('GPU') else "No GPU")

#model_handle = 'https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_xl/feature_vector/2'
model_handle = 'https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b0/feature_vector/2'
pixels = 224

print(f"Selected model:  {model_handle}")

IMAGE_SIZE = (pixels, pixels)
print(f"Input size {IMAGE_SIZE}")

BATCH_SIZE = 16

!unzip './flowers.zip'
data_dir = './flowers'

print(data_dir)

def build_dataset(subset):
  return tf.keras.preprocessing.image_dataset_from_directory(
      data_dir,
      validation_split=.20,
      subset=subset,
      label_mode="categorical",
      seed=123,
      image_size=IMAGE_SIZE,
      batch_size=1)

train_ds = build_dataset("training")
class_names = tuple(train_ds.class_names)
train_size = train_ds.cardinality().numpy()
train_ds = train_ds.unbatch().batch(BATCH_SIZE)
train_ds = train_ds.repeat()
normalization_layer = tf.keras.layers.Rescaling(1. / 255)
preprocessing_model = tf.keras.Sequential([normalization_layer])
train_ds = train_ds.map(lambda images, labels:
                        (preprocessing_model(images), labels))



val_ds = build_dataset("validation")
print(val_ds)
valid_size = val_ds.cardinality().numpy()
val_ds = val_ds.unbatch().batch(BATCH_SIZE)

val_ds = val_ds.map(lambda images, labels:
                    (normalization_layer(images), labels))

model = tf.keras.Sequential([
    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),
    hub.KerasLayer(model_handle, trainable=False),
    tf.keras.layers.Dropout(rate=0.1),
    tf.keras.layers.Dense(len(class_names),
                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))
])
model.build((None,)+IMAGE_SIZE+(3,))
model.summary()

model.compile(
  optimizer=tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9), 
  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.001),
  metrics=['accuracy'])

steps_per_epoch = train_size // BATCH_SIZE
validation_steps = valid_size // BATCH_SIZE
hist = model.fit(
    train_ds,
    epochs=5, steps_per_epoch=steps_per_epoch,
    validation_data=val_ds,
    validation_steps=validation_steps).history

x, y = next(iter(val_ds))
image = x[0, :, :, :]
true_index = np.argmax(y[0])
print(y[0])
print(true_index)
plt.imshow(image)
plt.axis('off')
plt.show()

prediction_scores = model.predict(np.expand_dims(image, axis=0))
predicted_index = np.argmax(prediction_scores)
print("True label: " + class_names[true_index])
print("Predicted label: " + class_names[predicted_index])

!mkdir './flowers_t'
!unzip './flowers_test.zip' -d './flowers_t'

img = plt.imread('./flowers_t/flowers_test/test_1.jpg')
plt.imshow(img)
img_sized = tf.image.resize(img, [224, 224])
img_sized.shape

images = []
images.append(img_sized)

prediction_scores = model.predict(np.expand_dims(img_sized, axis=0))
predicted_index = np.argmax(prediction_scores)
print("Predicted label: " + class_names[predicted_index])

img = plt.imread('./flowers_t/flowers_test/test_2.jpg')
plt.imshow(img)
img_sized = tf.image.resize(img, [224, 224])
img_sized.shape
images.append(img_sized)

prediction_scores = model.predict(np.expand_dims(img_sized, axis=0))
predicted_index = np.argmax(prediction_scores)
print("Predicted label: " + class_names[predicted_index])

img = plt.imread('./flowers_t/flowers_test/test_3.jpg')
plt.imshow(img)
img_sized = tf.image.resize(img, [224, 224])
img_sized.shape
images.append(img_sized)

prediction_scores = model.predict(np.expand_dims(img_sized, axis=0))
predicted_index = np.argmax(prediction_scores)
print("Predicted label: " + class_names[predicted_index])

img = plt.imread('./flowers_t/flowers_test/test_4.jpg')
plt.imshow(img)
img_sized = tf.image.resize(img, [224, 224])
img_sized.shape
images.append(img_sized)

print(len(images))

prediction_scores = model.predict(np.expand_dims(img_sized, axis=0))
predicted_index = np.argmax(prediction_scores)
print("Predicted label: " + class_names[predicted_index])

test_images = images
test_images = np.array(test_images)

print(test_images.shape)

import sklearn.preprocessing as preprocessing
classes = ['astilbe', 'bellflower', 'black-eyed susan', 'calendula', 'california poppy','carnation', 'common daisy', 'coreopsis', 'dandelion', 'iris', 'rose', 'sunflower', 'tulip']

targets = np.array(classes)
labelEnc = preprocessing.LabelEncoder()
new_target = labelEnc.fit_transform(targets)
onehotEnc = preprocessing.OneHotEncoder()
onehotEnc.fit(new_target.reshape(-1, 1))
targets_trans = onehotEnc.transform(new_target.reshape(-1, 1))
labels_enc = targets_trans.toarray()
print("The original data")
print(targets)
print("The transform data using OneHotEncoder")
print(labels_enc.shape)

test_labels = []
test_labels.append(labels_enc[6])
test_labels.append(labels_enc[12])
test_labels.append(labels_enc[12])
test_labels.append(labels_enc[10])
test_labels = np.array(test_labels)
print(test_labels)

print(test_images[0])

train_dataset = tf.data.Dataset.from_tensor_slices((test_images,test_labels)).batch(1)
train_dataset = train_dataset.map(lambda images, labels:
                        (preprocessing_model(images), labels))
print(train_dataset)
model.evaluate(train_dataset)

model.save('my_model.h5')
from google.colab import files
files.download('my_model.h5')